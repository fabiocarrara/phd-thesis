%============================= BACKGROUND =================================

% Often used notations
\def\x{\mathbf{x}} % input vector
\def\X{\mathbf{X}} % input dataset
\def\y{\mathbf{y}} % target/output vector
\def\Y{\mathbf{Y}} % target/output dataset
\def\w{\mathbf{w}} % weight vector
\def\W{\mathbf{W}} % weight matrix
\def\b{\mathbf{b}} % bias vector
\def\R{\mathbb{R}} % real numbers
\def\L{\mathcal{L}} % loss function
\def\a{\varphi} % activation function

\def\({\left (} % left par
\def\){\right )} % right par


\chapter{Background}
\label{ch:background}

In this chapter, we present the basic concepts about \gls{dl} and an overview of the research fields on which its application has been investigated in this thesis, namely Image Classification and \gls{cbir}.
The chapter is organized as follows.
In Section~\ref{sec:back:deep-learning}, we provide the reader with a quick introduction to \gls{dl}, focusing on deep neural networks for image and text processing and gradient-based optimization.
In Section~\ref{sec:back:image-classification}, an introduction to image classification using convolutional neural networks is presented together with a review of successful approaches in this field.
In Section~\ref{sec:back:image-retrieval}, we describe the main aspects of \gls{cbir} based on image representations extracted from deep neural networks, and we discuss some state-of-the-art methodologies to build effecive description of images and to efficiently index them in large-scale scenarios.
Section~\ref{sec:back:datasets} summarizes the public datasets used in the experiments presented in this thesis.


\section{Deep Learning}
\label{sec:back:deep-learning}

\acrfull{dl} defines the set of \gls{ml} methods aiming to learn from data a \emph{hierarchy of representations} specialized for the task under consideration~\cite{goodfellow2016deep}.
\gls{dl} models are usually organized as a sequence (or more generally a graph) of parametric non-linear transformations, known as \emph{layers}, that acts like features extractors;
starting from raw data, each layer searches for useful patterns in its input and provides higher-level representation of the data to the next layer.
More formally, given an input $\x$ and $L$ non-linear transformations $f_l\(\cdot; \theta_l\)$ parametrized by $\theta_l$ ($l=1, \dots, L$), we can express the output $\y$ of the cascade of transformations as:

\begin{align} \label{eq:back:deepnet}
    \y & = f\(\x, \Theta\) \\
       & = f_L\(\dots f_2\(f_1\(\x; \theta_1\); \theta_2\); \theta_L\)
\end{align}

where $\Theta = \{\theta_l, l = 1, \dots L\}$ indicates the set of all parameters, also known as \emph{weights}.

Given a training set $\X = \{\(\x_i, \y^\star_i\), i=1,\dots,N\}$ comprised by $N$ couples of inputs and desired outputs, the quality of a particular setting of parameters is quantitatively defined by a \emph{loss function} $\L\(\X; \Theta\)$ that measures how much predictions and targets differ;
the loss function is usually defined as the average of the individual loss values computed on each sample of the dataset:

\begin{align}
    \L\(\X; \Theta\) &= \frac{1}{N} \sum_{i=1}^N \mathcal{L}\(\y_i, \y^\star_i\) \\
                   &= \frac{1}{N} \sum_{i=1}^N \mathcal{L}\(f\(\x_i; \Theta\), \y^\star_i\)
\end{align}

% TODO add reference to loss definitions
where the particular formulation of $\L\(\y_i, \y^\star_i\)$ is task-dependent and further discussed in Section~\ref{}.
%In the learning phase, the model is optimized by changing the parameters $\Theta$ in such a way that the output $\mathbf{y}$ reflects the desired target $\mathbf{y^\star}$.
The learning problem is an optimization problem in which we search the best parameter setting $\Theta^\star$ that minimizes the loss function $\L\(\X; \Theta\)$:

\begin{equation} \label{eq:back:optim}
    \Theta^\star = \argmin_\Theta \L\(\X; \Theta\)
\end{equation}

For historical reasons, \gls{dl} models are also referred to as \emph{\glspl{dnn}} due to the resemblance of layers and their organization to the way neurons are interconnected and organized in the mammalian brain~\cite{}.  % TODO
\Glspl{dnn} can be roughly categorized in \glspl{ffnn}, in which information flows from input to output in a non-recursive cascade of computations, and \glspl{rnn}, which present a feedback loop in their computation graph.
In the following sections, we will review some practical and successful formulations of \glspl{dnn} in terms of their layers
%that are useful when dealing with image data
, and we will provide the reader with the basics of gradient-based optimization of~\eqref{eq:back:optim}.

\subsection{Feed-Forward Neural Network}
\label{subsec:back:ffnn}

\Acrlongpl{ffnn} are \gls{dl} models whose computation graph can be expressed as a directed acyclic graph, i.e.\,there are no feedback loops and information flows from inputs to outputs in a cascade fashion.
Thus, when computing of the whole chain from inputs to outputs, called the \emph{forward} pass of the network, each transformation defined by layers is computed only once.

In the following, we summarize some of the most relevant layers used in \glspl{dnn}.

\subsubsection{Fully Connected Layer}

The Fully Connected (or Inner Product) layer is a basic building block for \glspl{dnn}.
It performs a linear projection of the input followed by a usually non-linear element-wise activation function.
Formally, the output  $\y \in \R^m$ of the layer is obtained as follows:

\begin{equation} \label{eq:back:fully-connected}
    \y = \a \( \W \x + \b \)
\end{equation}

where $\x \in \R^n$ is the input data, $\W \in \R^{n \times m}$ and $\b \in \R^m$ are the parameters of a linear projection to a $m$-dimensional space.
% TODO add figure of activations
Commonly used activation functions $\a: \R \to \R$ are the \gls{relu}, the sigmoid $\sigma\(\cdot\)$, or $\tanh\(\cdot\)$ functions (see Figure~\ref{}).

Historically, this layer implemented a group of $m$ \emph{perceptrons}.
The perceptron~\cite{rosenblatt1958perceptron} is a biologically-inspired building block for artificial neural networks that has been developed mimicking the structure of biological neurons.
As a neuron cell, it is composed by $n$ inputs $\x_i$ ($i=1,\dots,n$), usually connected to the output of other neurons, and a single output (axon). % $\mathbf{y}$.
% TODO add neuron figure?
Each input connection is associated with a weight $\w_i$ ($i=1,\dots,n$) expressing how much of the signal coming through that connection is promoted or inhibited.
Weights in a perceptron are interpreted as the strength of the interconnection between neuron cells.
The neuron ``fires'' when the combination of their weighted inputs gets above a certain threshold -- this is modelled in the perceptron defining its output as the activation function $\a$ applied to the inner product between input and weights:

\begin{equation}
    \y_j = \a \( \sum_{i=1}^n \w_i \x_i + b\)
\end{equation}

where $\y_j$ is the output of a particular neuron and $b \in \R$ is an optional weight used as bias term.
% The weight vector $\w$ is tuned in the training phase in order to
A layer composed by $m$ perceptrons (and thus $m$ outputs $\y_j, j=1\dots m$) sharing the same input $\x$ can be formalized as~\eqref{eq:back:fully-connected}, where the columns of $\W$ correspond to the weights of the $m$ perceptrons.

Feed-forward artificial neural networks built stacking layers of perceptrons are called \glspl{mlp}.
In \glspl{mlp}, the outputs of a layer are densely connected to the input of each neuron comprising the next layer, hence the name ``fully-connected layer''.
A \gls{mlp} with $L$ layers can be defined as follows:

\begin{equation}
    \y = \a \( \W_L \( \dots \a \( \W_2 \( \a \( \W_1 \x + \b_1) \) + \b_2)\) + \b_L\)
\end{equation}

% TODO brutto finire con la formula

\subsubsection{Convolution Layer}

%Convolutional Neural Networks
%Recurrent Neural Networks
%LSTMs (Bidir)
%Loss Functions
%Cross-entropy
%MSE
%L2 Weight decay
%Gradient-Based Optimization

% TODO backprop (delta-rule) \cite{rumelhart1985learning}

%Backprop
%Optimizers: SGD (with momentum) / Adam
%Dropout (here?)

\section{Image Classification}
\label{sec:back:image-classification}

%Problem Setting
%Single-label binary- and multi-class image classification problem
%Examples (simple classif., sentiment analysis, etc.)
%Evaluation Metrics
%Top-k Accuracy
%AUC of ROC (TPR, FPR, confusion matrix)
%Recent Advances
%ILSVRC winners (Hybrid, VGG, Inception, Residual, ResNeXT, SENets?)
%Transfer Learning

%% probably move in the chapter
%Adversarial Examples for DNNs
%Definition
%Adv example formal definition
%Properties
%Adv. Generation Algorithms
%L-BFGS
%FGSM
%etc.
%Defense Strategies
%Change the net: Adversarial Training / other..
%Detect attacks: some rel. works on that


\section{Image Retrieval}
\label{sec:back:image-retrieval}

%Problem Setting
%CBIR (query-by-example)
%kNN schemes
%Image Representations
%Deep Features (fc7 -> RMAC)
%Permutation-based representations
%Deep Permutations
%Cross-media Retrieval
%Textual / visual / common space retrievals
%Datasets & Evaluation Metrics
%mAP, R@K, nDCG, medR, MRR

\setion{Datasets}
\label{sec:back:datasets}
%ILSVRC (+Places)?, (PKLot, CNRPark)? TwitTestDataset? T4SA?
%Holidays, Oxford, Paris + distr, COCO