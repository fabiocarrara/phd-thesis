%============================= INTRODUCTION =================================

\chapter{Introduction}
\label{ch:introduction}

% . vision probably the most important sense, convey much info fast
Vision is one of the primary senses of human beings, if not the most important one.
From generic signage to advertisements and artistic photography, imagery is ubiquitous in our lives due to its ability to convey lots of information quickly while overcoming cultural and linguistic barriers.
% . photos and videos everyday part of our lives, lots of data % -- ~13.37 img/day rita
Thanks to the ease of access to camera-equipped smartphones and social networks, we collectively generate, share, and receive a ridiculous amount of digital photos and videos daily.
According to InfoTrends\footnote{\url{http://blog.infotrends.com/}}, the number of digital photos taken worldwide in 2017 estimated at around 1.3 trillion, and the pace of digital media creation is intended to grow as more people get access to the Internet and cheap camera technology.
% -- problema organizzazione --> understanding automatico
% -- parallelo SE per testo <-> immagini
In lights of this scenario, there is an increasing interest in creating automatic tools for the management of digital visual data pursuing the same accessibility revolution textual search engines brought to the World Wide Web in the 90s and 00s.
Despite manual annotation and captioning of images and videos helped to build successful systems (e.g., keyword-based image search engines), methods relying on metadata surrounding visual data --- such as keywords, tags, captions --- require the creation of such metadata by human actors, which is a labor-intensive and subjective task that cannot scale to the current trend of digital media creation.
% on scales ranging from personal photo collections to big data companies with millions of users.
To overcome these limitations, the attention shifted to methods that try to model and infer the visual semantics in imagery relying solely upon the visual content, i.e., the information that machines can automatically extract from raw pixels and store it in numerical representations (image descriptors).
Early attempts to create image descriptors rely on low-level manually defined features of the images, such as the distribution of edges, colors, simple shapes, to name a few.
Most of the effort in these solutions is focused on manually defining the right combination of low-level features performing well for the specific task under analysis, which requires a considerable amount of domain expertise.

% . AI for vision in last years exploded
% -- img repre
% -- hand-crafted prima del 2012
% -- convnet e end2end grazie a ...
In the last years, a new wave in a field of Artificial Intelligence, called Deep Learning, enabled researchers to automatize perception and understanding of visual data by extracting information with high-level of abstraction from raw pixels, drastically limiting the human intervention in the process.
With the term Deep Learning, the research community indicates the family of Machine Learning techniques which aim to automatically learn from data a hierarchy of features extractors which map the input data in a high-level feature space tailored to a specific task to solve.
In the context of computer vision and image representation, Deep Learning, and in particular Convolutional Neural Networks, revolutionized feature engineering and visual understanding, outperforming handcrafted models on multiple vision tasks such as object recognition and detection, image description and retrieval,  and many more.
Convolutional Neural Networks are artificial neural networks specifically tailored to process image data and trained in a supervised end-to-end fashion.
Although CNNs have been around for many years, we pinpoint their turning point in 2012, year in which a deep convolutional neural network model outperformed approaches based on handcrafted features in the \acrlong{ilsvrc}.
Following this trend, the last six years experienced an overwhelming adoption of deep neural network models which set the new state of the art in numerous applications spanning multiple fields, including visual perception and image understanding.
This reborn of CNNs is attributed to multiple factors, the most important being the availability of large-scale datasets of labeled images (such as ImageNet) and the computational power offered by modern hardware accelerators such as GPUs.
Both of these factors contributed to training bigger models with millions of parameters which can achieve astonishing performance in challenging problems, such as fine-grained image classification including thousands of high-level concepts and semantics.

Nevertheless, deep-learning-based solutions pose non-trivial engineering challenges in their adoption.
In order to learn a functional hierarchy of features, models are defined to be deep, i.e., need to stack many parametric transformations (also called layers).
This not only considerably increases the computational budget for the model evaluation, but also increases the amount of supervision (in terms of the size of labeled data) needed to learn the parameters of the model properly.
The high computational budget drastically limits the applications of deep learning solutions in restricted environments with limited power resources, such as IoT devices and smartphones, which currently delegate complex data analysis to a centralized server.
Concerning training data, even if its creation is a one-time process, the manual labeling needed for its preparation still represents one of the highest cost of this kind of solutions.

In this thesis, ...
% . this thesis
% -- simplest formulation of image understanding (classif) -- limits
% --- engineering solutions to practical CNN problems in classif
% -- briding gap between technology of text and image search



