\chapter*{Summary}
\lettrine{T}{he} large diffusion of cheap cameras and smartphones led to an exponential daily production of digital visual data, such as images and videos.
In this context, most of the produced data lack manually assigned metadata needed for their manageability in large-scale scenarios, thus shifting the attention to the automatic understanding of the visual content.
Recent developments in Computer Vision and Artificial Intelligence empowered machines with high-level vision perception enabling the automatic extraction of high-quality information from raw visual data.
Specifically, \acrfullpl{cnn} provided a way to automatically learn effective representations of images and other visual data showing impressive results in vision-based tasks, such as image recognition and retrieval.

In this thesis, we investigated and enhanced the usability of \acrshortpl{cnn} for visual data management.
First, we identify three main limitations encountered in the adoption of \acrshortpl{cnn} and propose general solutions that we experimentally evaluated in the context of image classification.
We proposed miniaturized architectures to decrease the usually high computational cost of \acrshortpl{cnn} and enable edge inference in low-powered embedded devices.
%We evaluated our proposal in a practical distributed application, i.e., visual parking lot occupancy detection, extensively comparing the reduced models to state-of-the-art methods and architectures.
We tackled the problem of manually building huge training sets for models by proposing an automatic pipeline for training classifiers based on cross-media learning and Web-scraped weakly-labeled data.
We analyzed the robustness of \acrshortpl{cnn} representations to out-of-distribution data, specifically the vulnerability to adversarial examples, and proposed a detection method to discard spurious classifications provided by the model.
%
Secondly, we focused on the integration of \acrshort{cnn}-based \acrfull{cbir} in the most commonly adopted search paradigm, that is, textual search.
We investigated solutions to bridge the gap between image search and highly-developed textual search technologies by reusing both the front-end (text-based queries) and the back-end (distributed and scalable inverted indexes). % in \acrshort{cbir} scenarios.
We proposed a cross-modal image retrieval approach which enables textual-based image search on unlabeled collections by learning a mapping from textual to high-level visual representations.
Finally, we formalized, improved, and proposed novel surrogate text representations, i.e., text transcriptions of visual representations that can be indexed and retrieved by available textual search engines enabling \acrshort{cbir} without specialized indexes.
